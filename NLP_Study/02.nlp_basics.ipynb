{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd37e3c-8708-4a40-86ca-42cce7bce5d9",
   "metadata": {},
   "source": [
    "## NLP Components\n",
    "1. Lemmatization: Lemmatization is the process of reducing words to their base or dictionary form to normalize them across different forms. \n",
    "- Example: The lemma of the word \"running\" is \"run\".\n",
    "\n",
    "2. Token: A token is a single unit of text obtained after splitting a sentence or document based on certain criteria, such as whitespace or punctuation.\n",
    "- Example: In the sentence \"I love programming\", the tokens are \"I\", \"love\", and \"programming\".\n",
    "\n",
    "3. Stemming: Stemming is the process of reducing words to their root or base form by removing suffixes, which allows different variations of the same word to be treated as the same word.\n",
    "- Example: The stem of the words \"running\", \"runs\", and \"runner\" is \"run\".\n",
    "\n",
    "4. Stopwords: Stopwords are common words that are often filtered out during text processing because they typically do not carry significant meaning.\n",
    "- Example: In English, stopwords can include words like \"the\", \"is\", \"and\", etc.\n",
    "\n",
    "5. N-gram: An n-gram is a contiguous sequence of n items (words or characters) in a sentence or document.\n",
    "- Example: In the sentence \"The quick brown fox\", examples of n-grams are unigrams (\"The\", \"quick\", \"brown\", \"fox\"), bigrams (\"The quick\", \"quick brown\", \"brown fox\"), and trigrams (\"The quick brown\", \"quick brown fox\").\n",
    "\n",
    "6. TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF is a numerical statistic used to evaluate the importance of a word in a document relative to a collection of documents.\n",
    "- Example: In a document containing multiple sentences, the TF-IDF score of a word is higher if it appears frequently in that document but rarely in other documents in the collection.\n",
    "* $ W_{x,y} = {tf}_{x,y} * \\log({N \\over df_x} ) $\n",
    "* tf = frequency of x in y\n",
    "* df = number of documents containing x\n",
    "* N = total number of documents\n",
    "\n",
    "7. Tokenizer: A tokenizer is a tool used to break down a text into smaller units, such as words, phrases, or sentences, based on specific rules.\n",
    "- Example: The NLTK library in Python provides various tokenizers, such as word tokenizers and sentence tokenizers, which can be used to tokenize text data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f244cb-df4a-49c8-9066-7c61c49b04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dab80aa-7b70-4fbe-b2bf-f48eaa897606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CNN_Articels_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7491f3-0219-4e66-8f84-ed59ac8aff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date published</th>\n",
       "      <th>Category</th>\n",
       "      <th>Section</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/15/21 2:46</td>\n",
       "      <td>news</td>\n",
       "      <td>world</td>\n",
       "      <td>There's a shortage of truckers, but TuSimple t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/12/21 7:52</td>\n",
       "      <td>news</td>\n",
       "      <td>world</td>\n",
       "      <td>Bioservo's robotic 'Ironhand' could protect fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/16/21 2:51</td>\n",
       "      <td>news</td>\n",
       "      <td>asia</td>\n",
       "      <td>This swarm of robots gets smarter the more it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3/15/22 9:57</td>\n",
       "      <td>business</td>\n",
       "      <td>investing</td>\n",
       "      <td>Russia is no longer an option for investors. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3/15/22 11:27</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>Russian energy investment ban part of new EU s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>12/1/21 10:01</td>\n",
       "      <td>sport</td>\n",
       "      <td>tennis</td>\n",
       "      <td>Australian Open: Australia's vaccine mandate i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>12/1/21 17:56</td>\n",
       "      <td>sport</td>\n",
       "      <td>golf</td>\n",
       "      <td>Four golfers test positive ahead of South Afri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>12/1/21 11:32</td>\n",
       "      <td>sport</td>\n",
       "      <td>tennis</td>\n",
       "      <td>Peng Shuai: 'Unanimous conclusion' that tennis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>12/1/21 17:27</td>\n",
       "      <td>news</td>\n",
       "      <td>europe</td>\n",
       "      <td>This company is \"zapping\" cow dung with lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>12/1/21 14:32</td>\n",
       "      <td>sport</td>\n",
       "      <td>golf</td>\n",
       "      <td>Tiger Woods: Is this the end of his era? - CNN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4076 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date published  Category    Section  \\\n",
       "0      7/15/21 2:46      news      world   \n",
       "1      5/12/21 7:52      news      world   \n",
       "2      6/16/21 2:51      news       asia   \n",
       "3      3/15/22 9:57  business  investing   \n",
       "4     3/15/22 11:27  business   business   \n",
       "...             ...       ...        ...   \n",
       "4071  12/1/21 10:01     sport     tennis   \n",
       "4072  12/1/21 17:56     sport       golf   \n",
       "4073  12/1/21 11:32     sport     tennis   \n",
       "4074  12/1/21 17:27      news     europe   \n",
       "4075  12/1/21 14:32     sport       golf   \n",
       "\n",
       "                                                Article  \n",
       "0     There's a shortage of truckers, but TuSimple t...  \n",
       "1     Bioservo's robotic 'Ironhand' could protect fa...  \n",
       "2     This swarm of robots gets smarter the more it ...  \n",
       "3     Russia is no longer an option for investors. T...  \n",
       "4     Russian energy investment ban part of new EU s...  \n",
       "...                                                 ...  \n",
       "4071  Australian Open: Australia's vaccine mandate i...  \n",
       "4072  Four golfers test positive ahead of South Afri...  \n",
       "4073  Peng Shuai: 'Unanimous conclusion' that tennis...  \n",
       "4074  This company is \"zapping\" cow dung with lightn...  \n",
       "4075  Tiger Woods: Is this the end of his era? - CNN...  \n",
       "\n",
       "[4076 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8586a9fe-f788-468b-b58b-a92801ce71c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 17:42:43.883 python3[10786:4434808] Warning: Expected min height of view: (<NSButton: 0x7fe49e9f5030>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 17:42:50.877 python3[10786:4434808] Warning: Expected min height of view: (<NSButton: 0x7fe488015700>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2024-03-08 17:42:58.755 python3[10786:4434808] Warning: Expected min height of view: (<NSButton: 0x7fe4a0ba4250>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088b4e6b-bcbf-4439-9b78-24506f1e9cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 54] Connection\n",
      "[nltk_data]     reset by peer>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 54]\n",
      "[nltk_data]     Connection reset by peer>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a98dbfa-5408-47c8-819c-845a02d80cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_art = df['Article'].to_list()\n",
    "list_cat = df['Category'].to_list()\n",
    "list_sec = df['Section'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb40514-ad3c-4900-9cc2-c3346f8f5e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/td/1zp1k2xj2m5dwylk8w2lttg80000gn/T/ipykernel_10786/3488396418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Preprocess the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_art\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Corpus: \\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/td/1zp1k2xj2m5dwylk8w2lttg80000gn/T/ipykernel_10786/3488396418.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Preprocess the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_art\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Corpus: \\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/td/1zp1k2xj2m5dwylk8w2lttg80000gn/T/ipykernel_10786/3488396418.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-Z]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Tokenize the text into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and other non-alphanumeric characters\n",
    "    text =  re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Join the words back into a string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Preprocess the corpus\n",
    "corpus = [preprocess_text(doc) for doc in list_art]\n",
    "print('Corpus: \\n{}'.format(corpus))\n",
    "\n",
    "# Create a TfidfVectorizer object and fit it to the preprocessed corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the preprocessed corpus into a TF-IDF matrix\n",
    "tf_idf_matrix = vectorizer.transform(corpus)\n",
    "\n",
    "# Get list of feature names that correspond to the columns in the TF-IDF matrix\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(\"TF-IDF Matrix:\\n\",tf_idf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b84406-d186-4b48-96cb-3780aada3c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carparts",
   "language": "python",
   "name": "carparts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
