{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dfff8a-b35f-4247-ab32-c04a150d332f",
   "metadata": {},
   "source": [
    "## 1. BERT (Bidirectional Encoder Representations)\n",
    "https://www.analyticsvidhya.com/blog/2022/09/fine-tuning-bert-with-masked-language-modeling/\n",
    "- Deeper encoder stack with transformer architecture.\n",
    "- \n",
    "<div>\n",
    "<img src=\"03_images/04_bert_01.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## 2. MLM and NSP\n",
    "- MLM is pre-training or adaptation method. Some # of tokens are masked and the model is trained to predict the masked token.\n",
    "- NSP is that the model is trained with classification method. It is wether the second sentence is the next sentence or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f08f9a-280d-4cb9-a35c-3576660b43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers.utils import logging\n",
    "# logging.enable_progress_bar()\n",
    "# from transformers import logging\n",
    "\n",
    "# logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54902020-65f1-474b-be52-84f4a4f8709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('TalkFile_ner_2.csv.csv').iloc[:300,:]\n",
    "df['Tag'] = df['Tag'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdd1bc6-c653-4bbf-b2ca-821e1283706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_tag = df.Tag.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d97cdb8-6b04-4044-952e-d91699c7663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "list_labels = ['O'] + [i for i in list(set(chain.from_iterable(list_all_tag))) if i !='O']\n",
    "label2ind = {}\n",
    "ind2label = {}\n",
    "for ind,i in enumerate(list_labels):\n",
    "    label2ind[i]=ind\n",
    "    ind2label[ind]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba9ac40-175b-4043-a663-c41835c964d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'I-geo': 1,\n",
       " 'I-gpe': 2,\n",
       " 'I-art': 3,\n",
       " 'B-geo': 4,\n",
       " 'B-per': 5,\n",
       " 'B-gpe': 6,\n",
       " 'I-tim': 7,\n",
       " 'B-eve': 8,\n",
       " 'B-org': 9,\n",
       " 'I-org': 10,\n",
       " 'B-art': 11,\n",
       " 'I-per': 12,\n",
       " 'B-tim': 13,\n",
       " 'I-nat': 14,\n",
       " 'B-nat': 15,\n",
       " 'I-eve': 16}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b83883-22a3-4258-b29c-004441755b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sentence'].to_list()\n",
    "labels_ind_list = df['Tag'].apply(lambda x: \n",
    "                [label2ind[i] for i in x]\n",
    "               ).to_list()\n",
    "\n",
    "text_list = df['Sentence'].apply(lambda x:x.split(' ')).to_list()\n",
    "\n",
    "data_dict = {'id':list(range(len(text_list))),'tokens':text_list,'ner_tags':labels_ind_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93762283-52c6-4ddb-8ed4-4637b19d2aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 9, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             tokens  \\\n",
       "0   0  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1   1  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2   2  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3   3  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4   4  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0]  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 9, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(data_dict)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e54dd1-68a0-4d75-91d7-fec03ca04248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=17, id2label=ind2label, label2id=label2ind\n",
    ")\n",
    "for name, param in model.named_parameters():\n",
    "#     print(name)\n",
    "    if name.startswith(\"distilbert.embeddings\"):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff21add-ac97-4ad8-a9a6-231f834d072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13d16c7-f08f-40c7-bde6-553b7e3d7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71299b90-ad45-4d3a-916b-8985e71f532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df = train_test_split(new_df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165aab24-2207-4179-9f63-d2ab7b8d8095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                         | 0/240 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset_dict = datasets.DatasetDict()\n",
    "dataset_dict['train'] = datasets.Dataset.from_pandas(train_df)\n",
    "dataset_dict['test'] = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b05c11ef-90b2-4825-a84e-bc625cdc9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235a0c00-f4dc-46df-a73a-207ec6c8ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "import numpy as np\n",
    "\n",
    "labels = [ind2label[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [ind2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ind2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dfb4a09-2c91-47e9-8bf0-303bae824df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 240\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 01:44, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.819988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.506105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.404733</td>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.398601</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>0.894996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353480</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.489510</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.327936</td>\n",
       "      <td>0.493421</td>\n",
       "      <td>0.524476</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.916138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321240</td>\n",
       "      <td>0.471338</td>\n",
       "      <td>0.517483</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.917548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-15\n",
      "Configuration saved in ./checkpoint-15/config.json\n",
      "Model weights saved in ./checkpoint-15/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-15/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-15/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30/config.json\n",
      "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-45\n",
      "Configuration saved in ./checkpoint-45/config.json\n",
      "Model weights saved in ./checkpoint-45/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-45/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-45/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60/config.json\n",
      "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-75\n",
      "Configuration saved in ./checkpoint-75/config.json\n",
      "Model weights saved in ./checkpoint-75/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-75/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-75/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90/config.json\n",
      "Model weights saved in ./checkpoint-90/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-105\n",
      "Configuration saved in ./checkpoint-105/config.json\n",
      "Model weights saved in ./checkpoint-105/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-105/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-105/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint-105 (score: 0.32124000787734985).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=105, training_loss=0.6173745291573661, metrics={'train_runtime': 105.4581, 'train_samples_per_second': 15.93, 'train_steps_per_second': 0.996, 'total_flos': 18394411378944.0, 'train_loss': 0.6173745291573661, 'epoch': 7.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66874c98-0c6b-4077-b3a1-92382b6011e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json from cache at /Users/tchun/.cache/huggingface/transformers/9156cd487ebc07b22755262799b39fcdc0d5ae65bb62a1c8dc21ebe3f74bbf58.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-geo\",\n",
      "    \"2\": \"I-gpe\",\n",
      "    \"3\": \"I-art\",\n",
      "    \"4\": \"B-geo\",\n",
      "    \"5\": \"B-per\",\n",
      "    \"6\": \"B-gpe\",\n",
      "    \"7\": \"I-tim\",\n",
      "    \"8\": \"B-eve\",\n",
      "    \"9\": \"B-org\",\n",
      "    \"10\": \"I-org\",\n",
      "    \"11\": \"B-art\",\n",
      "    \"12\": \"I-per\",\n",
      "    \"13\": \"B-tim\",\n",
      "    \"14\": \"I-nat\",\n",
      "    \"15\": \"B-nat\",\n",
      "    \"16\": \"I-eve\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-art\": 11,\n",
      "    \"B-eve\": 8,\n",
      "    \"B-geo\": 4,\n",
      "    \"B-gpe\": 6,\n",
      "    \"B-nat\": 15,\n",
      "    \"B-org\": 9,\n",
      "    \"B-per\": 5,\n",
      "    \"B-tim\": 13,\n",
      "    \"I-art\": 3,\n",
      "    \"I-eve\": 16,\n",
      "    \"I-geo\": 1,\n",
      "    \"I-gpe\": 2,\n",
      "    \"I-nat\": 14,\n",
      "    \"I-org\": 10,\n",
      "    \"I-per\": 12,\n",
      "    \"I-tim\": 7,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/tchun/.cache/huggingface/transformers/1c7db6d4091181e587f1ecfbc4cfb993b3b1bbb00393f6046b324983a0e95cd9.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json from cache at /Users/tchun/.cache/huggingface/transformers/9156cd487ebc07b22755262799b39fcdc0d5ae65bb62a1c8dc21ebe3f74bbf58.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/vocab.txt from cache at /Users/tchun/.cache/huggingface/transformers/ee94cc601af0bf6dec7ee8d6ebee61d328a49ac32a3b5614afa6089b36db4fa1.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /Users/tchun/.cache/huggingface/transformers/8355126558c31df7994581710f04f9f05c756cde51b71d823ebfc36c841a3f14.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/tchun/.cache/huggingface/transformers/a4470a23a8bf132214b459824c85f4d1a07cd115fe751f3ba276968440bb9e96.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json from cache at /Users/tchun/.cache/huggingface/transformers/9156cd487ebc07b22755262799b39fcdc0d5ae65bb62a1c8dc21ebe3f74bbf58.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert_new/tokenizer_config.json\n",
      "Special tokens file saved in distilbert_new/special_tokens_map.json\n",
      "Didn't find file distilbert_new/added_tokens.json. We won't load it.\n",
      "loading file distilbert_new/vocab.txt\n",
      "loading file distilbert_new/tokenizer.json\n",
      "loading file None\n",
      "loading file distilbert_new/special_tokens_map.json\n",
      "loading file distilbert_new/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                         | 0/240 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "PyTorch: setting up devices                                       \n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 240\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 01:45, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.856560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.683308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.532425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.852008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.415180</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.237762</td>\n",
       "      <td>0.289362</td>\n",
       "      <td>0.881607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.349942</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.426573</td>\n",
       "      <td>0.463878</td>\n",
       "      <td>0.902044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319192</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.531469</td>\n",
       "      <td>0.552727</td>\n",
       "      <td>0.917548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.311327</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.531469</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.918252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-15\n",
      "Configuration saved in ./checkpoint-15/config.json\n",
      "Model weights saved in ./checkpoint-15/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-15/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-15/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30/config.json\n",
      "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-45\n",
      "Configuration saved in ./checkpoint-45/config.json\n",
      "Model weights saved in ./checkpoint-45/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-45/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-45/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60/config.json\n",
      "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-75\n",
      "Configuration saved in ./checkpoint-75/config.json\n",
      "Model weights saved in ./checkpoint-75/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-75/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-75/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90/config.json\n",
      "Model weights saved in ./checkpoint-90/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-105\n",
      "Configuration saved in ./checkpoint-105/config.json\n",
      "Model weights saved in ./checkpoint-105/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-105/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-105/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint-105 (score: 0.31132733821868896).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=105, training_loss=0.6600258963448661, metrics={'train_runtime': 106.0596, 'train_samples_per_second': 15.84, 'train_steps_per_second': 0.99, 'total_flos': 18394411378944.0, 'train_loss': 0.6600258963448661, 'epoch': 7.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tokenize_and_align_labels2(examples):\n",
    "    tokenized_inputs = tokenizer2(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "model2 = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=17, id2label=ind2label, label2id=label2ind\n",
    ")\n",
    "\n",
    "for name, param in model2.named_parameters():\n",
    "#     print(name)\n",
    "    if name.startswith(\"distilbert.embeddings\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "data = new_df['tokens'].to_list()\n",
    "tokenizer2.train_new_from_iterator(data,vocab_size=tokenizer.vocab_size)\n",
    "# Save the trained tokenizer\n",
    "tokenizer2.save_pretrained('distilbert_new')\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"distilbert_new\")\n",
    "\n",
    "import datasets\n",
    "dataset_dict = datasets.DatasetDict()\n",
    "dataset_dict['train'] = datasets.Dataset.from_pandas(train_df)\n",
    "dataset_dict['test'] = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels2, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "data_collator2 = DataCollatorForTokenClassification(tokenizer=tokenizer2)\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71be5c-cfc5-4f74-a84b-f2c1d0e5e3bb",
   "metadata": {},
   "source": [
    "### M3(MLM) + Trained_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d76ae0d-453d-4ca9-b09f-75a2e6bd795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling,DataCollatorForWholeWordMask\n",
    "\n",
    "\n",
    "class TokenizedSentencesDataset:\n",
    "  def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
    "      self.tokenizer = tokenizer\n",
    "      self.sentences = sentences\n",
    "      self.max_length = max_length\n",
    "      self.cache_tokenization = cache_tokenization\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "      if not self.cache_tokenization:\n",
    "          return self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "\n",
    "      if isinstance(self.sentences[item], str):\n",
    "          self.sentences[item] = self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
    "      return self.sentences[item]\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.sentences)\n",
    "max_length = 100\n",
    "mlm_prob=0.15\n",
    "train_dataset = TokenizedSentencesDataset(df['Sentence'].to_list()[:260], tokenizer2, max_length)\n",
    "dev_dataset = TokenizedSentencesDataset(df['Sentence'].to_list()[260:], tokenizer2, max_length, cache_tokenization=True) if len(df['Sentence'].to_list()[:260]) > 0 else None\n",
    "\n",
    "\n",
    "do_whole_word_mask = True\n",
    "if do_whole_word_mask:\n",
    "  data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer2, mlm=True, mlm_probability=mlm_prob)\n",
    "else:\n",
    "  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer2, mlm=True, mlm_probability=mlm_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9db62fb-ba5b-404e-822b-8326f4c89852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json from cache at /Users/tchun/.cache/huggingface/transformers/9156cd487ebc07b22755262799b39fcdc0d5ae65bb62a1c8dc21ebe3f74bbf58.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/tchun/.cache/huggingface/transformers/1c7db6d4091181e587f1ecfbc4cfb993b3b1bbb00393f6046b324983a0e95cd9.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert/distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 260\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34, training_loss=2.4421227399040673, metrics={'train_runtime': 51.3394, 'train_samples_per_second': 10.129, 'train_steps_per_second': 0.662, 'total_flos': 5733270001152.0, 'train_loss': 2.4421227399040673, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelForMaskedLM\n",
    "model3 = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \".\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_gpu_train_batch_size= 16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4387280d-84c5-462d-b844-8efd8ab67816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./saved_model3/config.json\n",
      "Model weights saved in ./saved_model3/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model3.save_pretrained('./saved_model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3bb0630-153c-4915-a553-b7218b915cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_model3/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"saved_model3\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-geo\",\n",
      "    \"2\": \"I-gpe\",\n",
      "    \"3\": \"I-art\",\n",
      "    \"4\": \"B-geo\",\n",
      "    \"5\": \"B-per\",\n",
      "    \"6\": \"B-gpe\",\n",
      "    \"7\": \"I-tim\",\n",
      "    \"8\": \"B-eve\",\n",
      "    \"9\": \"B-org\",\n",
      "    \"10\": \"I-org\",\n",
      "    \"11\": \"B-art\",\n",
      "    \"12\": \"I-per\",\n",
      "    \"13\": \"B-tim\",\n",
      "    \"14\": \"I-nat\",\n",
      "    \"15\": \"B-nat\",\n",
      "    \"16\": \"I-eve\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-art\": 11,\n",
      "    \"B-eve\": 8,\n",
      "    \"B-geo\": 4,\n",
      "    \"B-gpe\": 6,\n",
      "    \"B-nat\": 15,\n",
      "    \"B-org\": 9,\n",
      "    \"B-per\": 5,\n",
      "    \"B-tim\": 13,\n",
      "    \"I-art\": 3,\n",
      "    \"I-eve\": 16,\n",
      "    \"I-geo\": 1,\n",
      "    \"I-gpe\": 2,\n",
      "    \"I-nat\": 14,\n",
      "    \"I-org\": 10,\n",
      "    \"I-per\": 12,\n",
      "    \"I-tim\": 7,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file saved_model3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at saved_model3 were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at saved_model3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model4 = AutoModelForTokenClassification.from_pretrained(\n",
    "    'saved_model3', num_labels=17, id2label=ind2label, label2id=label2ind\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff0e99-bfd1-4c7d-a8f9-241b842cc0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices                                       \n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 240\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='98' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 98/105 01:57 < 00:08, 0.82 it/s, Epoch 6.47/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.852008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.384723</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.892882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.333579</td>\n",
       "      <td>0.519737</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>0.535593</td>\n",
       "      <td>0.914729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.309591</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>0.622378</td>\n",
       "      <td>0.589404</td>\n",
       "      <td>0.926709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-15\n",
      "Configuration saved in ./checkpoint-15/config.json\n",
      "Model weights saved in ./checkpoint-15/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-15/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-15/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30/config.json\n",
      "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-45\n",
      "Configuration saved in ./checkpoint-45/config.json\n",
      "Model weights saved in ./checkpoint-45/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-45/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-45/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60/config.json\n",
      "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-75\n",
      "Configuration saved in ./checkpoint-75/config.json\n",
      "Model weights saved in ./checkpoint-75/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-75/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-75/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, id, __index_level_0__, tokens. If ner_tags, id, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90/config.json\n",
      "Model weights saved in ./checkpoint-90/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for name, param in model4.named_parameters():\n",
    "#     print(name)\n",
    "    if name.startswith(\"distilbert.embeddings\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "import datasets\n",
    "dataset_dict = datasets.DatasetDict()\n",
    "dataset_dict['train'] = datasets.Dataset.from_pandas(train_df)\n",
    "dataset_dict['test'] = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels2, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model4,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f268aa-1da8-4ff7-b338-ee32dc402129",
   "metadata": {},
   "source": [
    "## 3. GPL\n",
    "- a semi-supervised learning technique that combines labeled data with pseudo-labeled data generated by a model to improve performance and leverage unlabeled data effectively.\n",
    "- Similar to NSP, but we generate the unlabeled data using the initial model and label it based on the confidence score or threshold and perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da5f3e-7fad-421a-8a84-c0e3df966eca",
   "metadata": {},
   "source": [
    "## 4.Reinforcement Learning\n",
    "<div>\n",
    "<img src=\"03_images/04_rl_01.png\" width=\"500\">\n",
    "</div>\n",
    "Reinforcement learning (RL) involves an agent interacting with an environment, taking actions in states to maximize cumulative rewards. The agent learns from feedback in the form of rewards received after each action, adjusting its decision-making policies to improve performance over time. RL aims to find an optimal strategy (policy) for the agent to make decisions that lead to the highest long-term rewards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carparts",
   "language": "python",
   "name": "carparts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
