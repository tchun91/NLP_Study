{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77202144-47d0-40ec-bebe-e362b6912dbd",
   "metadata": {},
   "source": [
    "1. Few shot learning/one shot learning and semi supervised learning, pair wise learning\n",
    "- One-shot learning/Few-shot learning: These techniques enable machine learning models to make accurate predictions or classifications with minimal training data, either from a single example (one-shot learning) or a small number of examples (few-shot learning).\n",
    "- Pairwise learning: Pairwise learning involves training models based on comparisons between pairs of data points, often used in tasks like ranking or preference learning.\n",
    "- Semi-supervised learning: Semi-supervised learning combines labeled and unlabeled data during training to improve model performance, particularly beneficial when labeled data is limited or expensive to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a45603-06cd-4564-9471-bae7d6f250a0",
   "metadata": {},
   "source": [
    "2. Siamese triplet loss\n",
    "- $ L=max(0,d(A,P)−d(A,N)+α) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82365f1e-7a91-48f4-a02b-d615635b082b",
   "metadata": {},
   "source": [
    "Contrastive Loss\n",
    "$  L = \\frac{1}{2N} \\sum_{i=1}^{N} \\left( y_i \\cdot d_i^2 + (1 - y_i) \\cdot \\max(margin - d_i, 0)^2 \\right)  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b233130-8600-4fb8-a091-d1c9e096c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ed846a-5c98-4a0e-8426-56b2221b1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('TalkFile_ner_2.csv.csv').iloc[:300,:]\n",
    "df['Tag'] = df['Tag'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b609e3-64d2-4160-b041-3cb63b1d21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_tag = df.Tag.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19f6bd6-c4f1-431c-acb1-a64affd462d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "list_labels = ['O'] + [i for i in list(set(chain.from_iterable(list_all_tag))) if i !='O']\n",
    "label2ind = {}\n",
    "ind2label = {}\n",
    "for ind,i in enumerate(list_labels):\n",
    "    label2ind[i]=ind\n",
    "    ind2label[ind]=i\n",
    "# df['Sentence'].to_list()\n",
    "labels_ind_list = df['Tag'].apply(lambda x: \n",
    "                [label2ind[i] for i in x]\n",
    "               ).to_list()\n",
    "\n",
    "text_list = df['Sentence'].apply(lambda x:x.split(' ')).to_list()\n",
    "\n",
    "data_dict = {'id':list(range(len(text_list))),'tokens':text_list,'ner_tags':labels_ind_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b28560f-4382-44ee-9cc6-f811acaa7f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 11, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 9, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 3,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             tokens  \\\n",
       "0   0  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1   1  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2   2  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3   3  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4   4  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 11, 0, 0...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 9, 0]  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 3,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(data_dict)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6889288e-b7a8-49e4-93a1-bbcdc6ece360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['tokens'].apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6083ae6-c9f7-4238-b061-d9c2d50b2855",
   "metadata": {},
   "source": [
    "### T5 Similarity Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72129fa-17c2-4638-aa8c-fcda19911489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a88f3f15-ebd8-4298-8ce6-ce8817c468ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9267)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and encode the input texts\n",
    "inputs = tokenizer(df['Sentence'].to_list()[:260], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Generate embeddings using T5 model (encoder outputs)\n",
    "with torch.no_grad():\n",
    "    outputs = model.encoder(**inputs, output_hidden_states=True)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Use embeddings from the last layer of the encoder\n",
    "\n",
    "# Calculate cosine similarity using PyTorch's cosine similarity function\n",
    "similarity = torch.nn.functional.cosine_similarity(embeddings[1,:], embeddings[5,:], dim=0)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c5baca-4d23-485b-b5fc-7196469adf2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>225</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>210</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>258.0</td>\n",
       "      <td>188</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>258.0</td>\n",
       "      <td>235</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>259.0</td>\n",
       "      <td>63</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>259.0</td>\n",
       "      <td>132</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>259.0</td>\n",
       "      <td>67</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anchor  pos  neg\n",
       "0       0.0   79  208\n",
       "0       0.0  225   17\n",
       "0       0.0   49   13\n",
       "1       1.0   31  240\n",
       "1       1.0  210  225\n",
       "..      ...  ...  ...\n",
       "258   258.0  188   87\n",
       "258   258.0  235  117\n",
       "259   259.0   63  174\n",
       "259   259.0  132  253\n",
       "259   259.0   67  103\n",
       "\n",
       "[780 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_triplet = pd.DataFrame({'anchor':[],'pos':[],'neg':[]})\n",
    "for i in range(embeddings.shape[0]):\n",
    "    similarity = torch.nn.functional.cosine_similarity(embeddings[i,:], embeddings[:,:], dim=1)\n",
    "    pos_q1 = torch.quantile((similarity),0.7)\n",
    "    pos_q2 = torch.quantile((similarity),0.9)\n",
    "    neg_q1 = torch.quantile((similarity),0.1)\n",
    "    neg_q2 = torch.quantile((similarity),0.3)\n",
    "    pos_ts = ((similarity>=pos_q1)&(similarity<=pos_q2)).nonzero().view(-1)\n",
    "    neg_ts = ((similarity>=neg_q1)&(similarity<=neg_q2)).nonzero().view(-1)\n",
    "    df_triplet = df_triplet.append({'anchor':int(i),'pos':pos_ts.tolist(),'neg':neg_ts.tolist()},ignore_index=True)\n",
    "\n",
    "import random\n",
    "df_triplet['pos'] = df_triplet['pos'].apply(lambda x: random.sample(x,k=3))\n",
    "df_triplet['neg'] = df_triplet['neg'].apply(lambda x: random.sample(x,k=3))\n",
    "\n",
    "neg_list = df_triplet.explode('neg')['neg']\n",
    "df_triplet = df_triplet.explode('pos')\n",
    "df_triplet['neg']=neg_list\n",
    "df_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7200c288-b30a-4d3a-a2cc-6a63377cacf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>Sudan 's government says it will order troops ...</td>\n",
       "      <td>Thai Deputy Interior Minister Sutham Saengprat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>There are only about 1,600 pandas still living...</td>\n",
       "      <td>Poor residents often complain they have been c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              anchor  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Thousands of demonstrators have marched throug...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  Sudan 's government says it will order troops ...   \n",
       "1  There are only about 1,600 pandas still living...   \n",
       "\n",
       "                                                 neg  \n",
       "0  Thai Deputy Interior Minister Sutham Saengprat...  \n",
       "1  Poor residents often complain they have been c...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_triplet_s = pd.DataFrame({'anchor':[],'pos':[],'neg':[]})\n",
    "for ind,i in df_triplet.iterrows():\n",
    "    df_triplet_s = df_triplet_s.append({\n",
    "        'anchor':df['Sentence'][i[0]],\n",
    "        'pos':df['Sentence'][i[1]],\n",
    "        'neg':df['Sentence'][i[2]]\n",
    "    },ignore_index=True)\n",
    "df_triplet_s.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0031c3-2f83-4bcf-8871-e9357cf25f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TokenizedSentencesDataset(Dataset):\n",
    "  def __init__(self, sentences, tokenizer, max_length):\n",
    "      self.tokenizer = tokenizer\n",
    "      self.sentences = sentences\n",
    "      self.max_length = max_length\n",
    "\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    anchor_tok= self.tokenizer(self.sentences['anchor'].to_list()[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True, return_tensors='pt',padding='max_length')\n",
    "    pos_tok= self.tokenizer(self.sentences['pos'].to_list()[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True, return_tensors='pt',padding='max_length')\n",
    "    neg_tok= self.tokenizer(self.sentences['neg'].to_list()[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True, return_tensors='pt',padding='max_length')\n",
    "    return anchor_tok,pos_tok,neg_tok\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.sentences['anchor'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3422c10c-8bcb-44d5-965a-cd04f3577831",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "train_dataset = TokenizedSentencesDataset(df_triplet_s, tokenizer, max_length)\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset,batch_size=50,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc9099-ea22-4691-9b1f-fa10266b2fae",
   "metadata": {},
   "source": [
    "### Train using distilbert and biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81113ab7-ca50-46da-81d2-8876d78b5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad41190b-61ef-4b0c-b85e-39f064c058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True,bidirectional = True)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).squeeze(dim=1)\n",
    "        _, (h, c) = self.lstm(x)\n",
    "        return h[-1]\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, num_layers)\n",
    "    def forward(self, x1, x2,x3):\n",
    "        h1 = self.encoder(x1)\n",
    "        h2 = self.encoder(x2)\n",
    "        h3 = self.encoder(x3)\n",
    "        return h1, h2, h3\n",
    "    def get_embeddings(self, x1):\n",
    "        anchor = self.encoder(x1)\n",
    "        return anchor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39686a01-6a88-476d-9eff-079dc7cfcd1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/td/1zp1k2xj2m5dwylk8w2lttg80000gn/T/ipykernel_35487/3271811113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/td/1zp1k2xj2m5dwylk8w2lttg80000gn/T/ipykernel_35487/1755301393.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, x3)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/td/1zp1k2xj2m5dwylk8w2lttg80000gn/T/ipykernel_35487/1755301393.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbidirectional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Define Loss Function\n",
    "loss_function = nn.TripletMarginLoss(margin=1.0)\n",
    "\n",
    "# Step 2: Prepare Data (Assuming triplets are prepared)\n",
    "\n",
    "# Step 3: Instantiate Model and Loss Function\n",
    "input_size = len(tokenizer)\n",
    "hidden_size = 768  # Example hidden size\n",
    "num_layers = 2    # Example number of layers\n",
    "siamese_model = SiameseNetwork(input_size, hidden_size, num_layers)\n",
    "optimizer = optim.Adam(siamese_model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 4: Training Loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    siamese_model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    \n",
    "    for batch in dataloader_train:\n",
    "        anchor, positive, negative = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h1, h2, h3 = siamese_model(anchor['input_ids'], positive['input_ids'], negative['input_ids'])\n",
    "        \n",
    "        loss = loss_function(h1, h2, h3)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(dataloader_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8f381-ed86-47a5-af73-fede9300f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in siamese_model.named_parameters():\n",
    "    print(i[0],i[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2ec6d-f748-47fb-936a-835996624101",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reassign the pre-trained weights to new model with linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77c92e-320c-4813-8555-bcd0524d1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_lin(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,num_classes):\n",
    "        super(Encoder_lin, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True,bidirectional = True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(hidden_size*2,num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).squeeze(dim=1)\n",
    "        h,_ = self.lstm(x)\n",
    "        dropout_seq = self.dropout(h)\n",
    "\n",
    "        logits = self.linear(dropout_seq)\n",
    "        \n",
    "        return logits\n",
    "num_classes = 17\n",
    "elin = Encoder_lin(input_size, hidden_size, num_layers,num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb53c05-0e7d-4eb1-9f8d-6358584091b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state_dict = elin.state_dict()\n",
    "siamese_state_dict = siamese_model.encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73bb1d-0573-4551-a2c1-1e53169faa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights in encoder_lin_model with weights from siamese_state_dict\n",
    "for name, param in siamese_state_dict.items():\n",
    "    if name.startswith('lstm') or name.startswith('embedding'):  # Exclude linear layer weights\n",
    "        print(name)\n",
    "        encoder_state_dict[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf051c8c-83c0-4be9-af04-7163e8d7f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elin.load_state_dict(encoder_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a32994-9ee4-4252-9727-aade8fa05036",
   "metadata": {},
   "source": [
    "# NER Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7405ec-6d02-4cd8-8f8c-46813cb77f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['Sentence'].to_list()\n",
    "labels_ind_list = df['Tag'].apply(lambda x: \n",
    "                [label2ind[i] for i in x]\n",
    "               ).to_list()\n",
    "\n",
    "text_list = df['Sentence'].apply(lambda x:x.split(' ')).to_list()\n",
    "\n",
    "data_dict = {'id':list(range(len(text_list))),'tokens':text_list,'ner_tags':labels_ind_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011c713-a969-40ce-9542-ce8b19147de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(data_dict)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f47a4d-7fdd-4d18-80c6-1424126df49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_padding = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473cc61-6125-4231-823d-553891ae3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "new_df_padding['ner_tags'] = new_df_padding.ner_tags.apply(lambda x: x + [0 for i in range(max_length-len(x))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5af09f-d0f6-4d7d-be96-d2454f8e97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df = train_test_split(new_df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22025fc8-a38c-4fc9-83c8-36e0ad2f94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch Dataset\n",
    "class token_label_dataset(Dataset):\n",
    "    def __init__(self, dataframe,tokenizer,max_length=50,num_class = 17):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer= tokenizer\n",
    "        self.num_class= num_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.tokenizer(' '.join(self.dataframe.iloc[idx]['tokens']), add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True, return_tensors='pt',padding='max_length')\n",
    "        labels = torch.tensor(self.dataframe.iloc[idx]['ner_tags'], dtype=torch.long)\n",
    "        padded_labels = torch.nn.functional.pad(labels, (0, self.max_length - len(labels)))\n",
    "        label_f = torch.nn.functional.one_hot(padded_labels, num_classes=self.num_class).permute(0, 1).float()\n",
    "        len_tok = len(self.dataframe.iloc[idx]['tokens'])\n",
    "        return text['input_ids'].squeeze(), label_f,len_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984669d-efea-42c6-be1f-efa9afa36ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=50\n",
    "train_dataset2 = token_label_dataset(train_df,tokenizer,max_length,num_class=17)\n",
    "test_dataset2 = token_label_dataset(test_df,tokenizer,max_length,num_class=17)\n",
    "\n",
    "train_DL2 = DataLoader(train_dataset2,batch_size=50,shuffle=True)\n",
    "test_DL2 = DataLoader(test_dataset2,batch_size=50,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8872b3e-58b7-476d-9f44-183037def7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids, labels,len_tok = batch\n",
    "            input_ids = input_ids\n",
    "            labels = labels\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            _, predicted = torch.max(logits, dim=2)  # Get predicted class indices\n",
    "            _, labels = torch.max(labels, dim=2)  # Get predicted class indices\n",
    "\n",
    "            max_len_tok = len_tok.max().tolist()\n",
    "            # Compute accuracy for this batch\n",
    "            correct_predictions += torch.sum(predicted[:,:max_len_tok] == labels[:,:max_len_tok]).item()\n",
    "            total_predictions += labels.numel()  # Total number of elements\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905963f8-1291-481c-a3ec-5564e3339979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom training function\n",
    "def train_custom(model, train_loader, optimizer, loss_fn, eval_loader, num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            input_ids, labels,len_tok = batch\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        eval_accuracy = evaluate_model(model, eval_loader)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, Eval Accuracy: {eval_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bb48d-a298-49ef-9e5a-be1de2984610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(elin.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "train_custom(elin, train_DL2, optimizer, loss_fn,test_DL2, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe4b08-aca8-4709-bed5-0787947b0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = 31"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carparts",
   "language": "python",
   "name": "carparts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
