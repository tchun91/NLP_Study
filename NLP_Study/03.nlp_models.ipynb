{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393104da-67f2-492b-a0f5-169ef6753a66",
   "metadata": {},
   "source": [
    "## 1. GAN (generative adversarial networks)\n",
    "- gan - https://www.geeksforgeeks.org/generative-adversarial-network-gan/\n",
    "\n",
    "- Unsupervised learning\n",
    "<div>\n",
    "<img src=\"03_images/03_gan_01.png\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "1. Architecture <br>\n",
    "1-1) Generator : it takes random noise as input and tries to generate data samples similar to real data from the target dist.<br>\n",
    "1-2) Discriminator : distinguish between real data from the target distribution and fake data made by generator.<br>\n",
    "2. Training Process <br>\n",
    "During training, the generator and discriminator are trained simultaneously in a minimax game framework.\n",
    "The generator aims to produce data samples that are indistinguishable from real data, while the discriminator aims to correctly classify real and fake samples.\n",
    "3. Objective<br>\n",
    "The objective of the generator is to minimize the probability that the discriminator correctly classifies its generated samples as fake.<br>\n",
    "The objective of the discriminator is to maximize the probability of correctly classifying real samples as real and fake samples as fake.<br>\n",
    "4. Adversarial Training <br>\n",
    "The training process involves alternating between updating the generator to fool the discriminator and updating the discriminator to better distinguish between real and fake samples.<br>\n",
    "This adversarial training process leads to a competitive dynamic where the generator improves at generating realistic samples, while the discriminator becomes more discerning.\n",
    "5. Loss Functions<br>\n",
    "The generator's loss function is typically the negative log likelihood of the discriminator being correct (i.e., minimizing the probability of the discriminator classifying its generated samples as fake).<br>\n",
    "The discriminator's loss function is typically a combination of the log likelihood of correct classification for real and fake samples (e.g., cross-entropy loss).\n",
    "6. Convergence<br>\n",
    "Ideally, GANs converge when the generator produces samples that are indistinguishable from real samples, and the discriminator is unable to differentiate between real and fake samples with high confidence.<br>\n",
    "7. Applications<br>\n",
    "GANs have been successfully applied in various domains, including image generation, image-to-image translation, text-to-image synthesis, style transfer, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667db9d1-84d3-4d9c-bd7d-7f7d3f23db04",
   "metadata": {},
   "source": [
    "## 2. Transformer, Sentence transformer\n",
    "- https://serokell.io/blog/transformers-in-ml\n",
    "- https://www.columbia.edu/~jsl2239/transformers.html\n",
    "- Encoder(Input into vectors) and Decoder(vectors into output)\n",
    "1) Encoder:\n",
    "It was introduced after we have seen the decoder cannot be passed from a hidden state beccause of the varying sizes of elements. (Also, rnn process input sequentially)\n",
    "<br>\n",
    "1-1) Multi-head self-attention : \n",
    "<br>\n",
    "This is to get the dependencies of all the tokens. Self attention access to each token in the sequence to all other tokens.\n",
    "<br>It splits the embeddings into mutiple heads, attention computations independently then concatenate the outputs (Hidden state).\n",
    "<br>\n",
    "<br>\n",
    "1-2) Position-wise Feedforward Networds:\n",
    "<br>\n",
    "each token -> position-wise feedforward networks. Fully connected + ReLU.\n",
    "<br>\n",
    "1-3) Layer Norm and Residual Connections:\n",
    "<br>\n",
    "Norm improves the stability and convergence.\n",
    "<br>\n",
    "Residual connections enable the gradients by adding the original input to the output of each sub-layer.\n",
    "** Attention : Q(input query),K(info from input compared to query; key),V(content associated with each key; value) vectors. The attention scores use some similarity measure(dot products, additive attention or socring function) to get the relationship between key and query vectors. Higher attention weight means higher relevancy to the query.\n",
    "\n",
    "2) Decoder:\n",
    "To get the output from encoded input representations. It attends encoder's output + previously generated tokens during decoding.\n",
    "<br>\n",
    "2-1) Masked Multi-Head Self-Attention Mechanism\n",
    "<br>\n",
    "Masking enables only access to the current input which is for short term understanding for the output.\n",
    "<br>\n",
    "2-2) Encoder-Decoder Attention\n",
    "<br>\n",
    "access to encoder output, helps decoder focuses on relevant parts of input sequence for the output; each token.\n",
    "<br>\n",
    "2-3) Position-wise Feedforward Networks and Output Projection\n",
    "<br>\n",
    "2-4) Layer Normalization and Residual Connections\n",
    "\n",
    "Apendix\n",
    "<br>\n",
    "Attention(decoder) - Helps to understand the context with before/after words (Bark for dog or tree bark).\n",
    "<br>\n",
    "Self-attention(encoder) - By capturing a set of attention weights which indicates the relevance of each element to every other element. (Long range sequence)\n",
    "<br>\n",
    "Multi-head attention - tokens are broken up into multiple parts (head) and go to attention computing process. It is concatenated to output.\n",
    "<br>\n",
    "These processes are in parallel.\n",
    "\n",
    "- Language models use a decoder-only architecture\n",
    "- BERT-style models use an encoder-only architecture\n",
    "\n",
    "<div>\n",
    "<img src=\"03_images/03_transformer_01.png\" width=\"500\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91cc49-58f2-41cb-8cc5-4c679aa45c0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. LSTM\n",
    "-  https://www.theaidream.com/post/introduction-to-rnn-and-lstm\n",
    "\n",
    "<div>\n",
    "<img src=\"03_images/03_lstm_01.png\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### Forward Propagation\n",
    "- Hidden state activation -> by the other local activations near ->> Short term memory\n",
    "- Activation state -> weights -> Long Term Memory\n",
    "- Cell state : conveyor belt that runs the entire chain only with linear interactions *Unchanged.\n",
    "\n",
    "<div>\n",
    "<img src=\"03_images/03_lstm_02.png\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- Gates structure regulates and controlls (Sigmoid layer by multiplication operation 1 or 0) information to the cell state. X is \"no\" and + is \"add\" in the diagram\n",
    "\n",
    "#### Cell state takes care of \"Long term memory\"\n",
    "\n",
    "#### Forget gate : It is a gate to cell state (long term). It determines whether to keep or not in the long term memory.\n",
    "#### Input gate : It is a gate to cell state (long term). It determines whether to add or not in the long term memory.\n",
    "#### Output gate : With tanh, the output gate represents the hidden state which contains both LSTM. W/o tanh, it represents the short term memory unfiltered. It goes to both cell gate, hidden state, and the output of the model at the last hidden state.\n",
    "\n",
    "\n",
    "## 4. RNN\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"03_images/03_rnn_01.png\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "- The image showing that previous hidden state which was made from previous input is used for the next hidden state and it affects the output. The previous input and current input are dependent of each other.\n",
    "\n",
    "- $ h_t = f(h_{t-1},x_t) $ where each input is called time step.\n",
    "- RNN has a very simple structure where previous input data affects current hidden state through previous state.\n",
    "- tanh is normally used for RNN because gradients can be biased towards positive or negative without hyper tangent. It helps faster learning by preventing those biases. $ h_t = tanh(W_{hh}h_{h-1} + W_{xh}x_t) $\n",
    "\n",
    "### Backward propagation \n",
    "- It also gets affected by previous steps. \n",
    "- It seems the current state has very strong relationship with past states, but, since only direct connection is from the previous state, it lacks the long term dependencies. Which usually RNN leads vanishing/exploding gradient problems.\n",
    "\n",
    "#### Vanishing problem\n",
    "- Because of the chain rules applied to multiple times over and over through states, if one gradient approaches to 0, everything else will approaches 0 too. (Vanishing problem)\n",
    "\n",
    "#### Exploding\n",
    "- We can clip the gradient when it rushes to greater than 1 due to overflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b85c30-f99f-4863-a9b4-0f906986b7bf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. biLSTM\n",
    "https://www.geeksforgeeks.org/bidirectional-lstm-in-nlp/\n",
    "\n",
    "- It is two LSTM layers; one processing the input in the forward direction and the other for backward direction.\n",
    "- This characteristic helps to better understand preceding and following words in a sentence.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"03_images/03_bilstm_01.png\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "- Advantage of BiLSTM is it has reduced vanishing gradient and higher accuracy, but has higher computation requirements due to the both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2857011-3099-4363-97c8-e15e08a83ad8",
   "metadata": {},
   "source": [
    "## 6. t5\n",
    "\n",
    "It has encoder-decoder architecture to solve many of different language problems.\n",
    "\n",
    "<div>\n",
    "<img src=\"03_images/03_t5_01.png\" width=\"700\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e433a-74aa-451b-b5ea-9621fdaec0cb",
   "metadata": {},
   "source": [
    "## 7. Tokenizer\n",
    "https://huggingface.co/learn/nlp-course/en/chapter6/1?fw=pt\n",
    "\n",
    "1) Byte-Pair Encoding (BPE):\n",
    "\n",
    "Overview: BPE is a data compression algorithm that has been adapted for subword tokenization in NLP. It works by iteratively merging the most frequent pairs of consecutive bytes (characters or subwords) in a corpus, effectively building a vocabulary of subword units.\n",
    "Tokenization Process:\n",
    "Start with a vocabulary containing individual characters or pre-defined subwords.\n",
    "Iteratively merge the most frequent pair of tokens in the corpus until reaching the desired vocabulary size or a predefined number of merge operations.\n",
    "The resulting vocabulary consists of subword units that can represent both frequent and rare words in the corpus.\n",
    "Example: Suppose we start with a vocabulary containing individual characters and apply BPE to the word \"apple.\" After several merge operations, BPE might produce subword units like \"ap,\" \"pl,\" and \"e\" in its vocabulary.\n",
    "\n",
    "\n",
    "2) WordPiece:\n",
    "\n",
    "Overview: WordPiece is a subword tokenization technique similar to BPE but with a different merging strategy. It was popularized by Google's BERT (Bidirectional Encoder Representations from Transformers) model. WordPiece operates by greedily merging the most frequent subword or character sequences in the corpus.\n",
    "Tokenization Process:\n",
    "Start with a vocabulary containing individual characters or pre-defined subwords.\n",
    "Greedily merge the most frequent subword or character sequences until reaching the desired vocabulary size or a predefined number of merge operations.\n",
    "The resulting vocabulary consists of subword units that can represent words of varying frequencies in the corpus.\n",
    "Example: Using the same word \"apple,\" WordPiece might produce subword units like \"app,\" \"le,\" and \"#\" (word boundary symbol) in its vocabulary.\n",
    "\n",
    "\n",
    "3) Unigram Tokenization:\n",
    "\n",
    "Overview: Unigram Tokenization is a subword tokenization technique that focuses on preserving the statistical properties of the corpus, particularly the frequency distribution of subword units. It aims to minimize the loss of information during tokenization.\n",
    "Tokenization Process:\n",
    "Start with a vocabulary containing individual characters or pre-defined subwords.\n",
    "Greedily select the most frequent subword units according to their occurrence in the corpus, while considering their individual tokenization costs (based on their frequencies).\n",
    "The resulting vocabulary consists of subword units that optimize the trade-off between representing frequent words accurately and minimizing the overall tokenization cost.\n",
    "Example: For \"apple,\" Unigram Tokenization might select subword units like \"ap,\" \"pp,\" \"l,\" and \"e\" based on their frequencies and tokenization costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666b71f8-3b5a-41c8-a18d-e481a6d6acc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffe8091-70ba-4174-8245-b923a6e54ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('TalkFile_ner_2.csv.csv')\n",
    "df['Tag'] = df['Tag'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d1c025-0d28-4542-8936-e20bb64f8f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                           Sentence  \\\n",
       "0  Sentence: 1  Thousands of demonstrators have marched throug...   \n",
       "1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n",
       "2  Sentence: 3  They marched from the Houses of Parliament to ...   \n",
       "3  Sentence: 4  Police put the number of marchers at 10,000 wh...   \n",
       "4  Sentence: 5  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n",
       "2  ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...   \n",
       "3  ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...   \n",
       "4  ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4b904c-7d8f-46fb-a2c3-848f88cfe48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_tag = df.Tag.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8868aa72-2c25-459f-8596-bc7853357dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "list_labels = ['O'] + [i for i in list(set(chain.from_iterable(list_all_tag))) if i !='O']\n",
    "label2ind = {}\n",
    "ind2label = {}\n",
    "for ind,i in enumerate(list_labels):\n",
    "    label2ind[i]=ind\n",
    "    ind2label[ind]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c17612-86db-483f-8bab-c6a7df4340f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-org': 2,\n",
       " 'I-tim': 3,\n",
       " 'B-nat': 4,\n",
       " 'I-geo': 5,\n",
       " 'I-org': 6,\n",
       " 'I-art': 7,\n",
       " 'I-eve': 8,\n",
       " 'B-per': 9,\n",
       " 'B-gpe': 10,\n",
       " 'B-art': 11,\n",
       " 'B-eve': 12,\n",
       " 'I-gpe': 13,\n",
       " 'I-nat': 14,\n",
       " 'I-per': 15,\n",
       " 'B-tim': 16}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce33bd45-c7e1-49ee-a254-f4e6ab3ee50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sentence'].to_list()\n",
    "labels_ind_list = df['Tag'].apply(lambda x: \n",
    "                [label2ind[i] for i in x]\n",
    "               ).to_list()\n",
    "\n",
    "text_list = df['Sentence'].apply(lambda x:x.split(' ')).to_list()\n",
    "\n",
    "data_dict = {'id':list(range(len(text_list))),'tokens':text_list,'ner_tags':labels_ind_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e615006-a781-4ead-86eb-3a6df374ed40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             tokens  \\\n",
       "0   0  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1   1  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2   2  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3   3  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4   4  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0]  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(data_dict)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0883e93-8b30-4ab9-88a9-916fc32dd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=17, id2label=ind2label, label2id=label2ind\n",
    ")\n",
    "model2 = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=17, id2label=ind2label, label2id=label2ind\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb51953-692f-48e0-8a9f-be425c0b6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8a8c21-612e-4fd4-8b70-bba391a624e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df = train_test_split(new_df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577d58b3-6173-42df-9f3c-40ff05a1d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                         | 0/38367 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "                                                                    \r"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset_dict = datasets.DatasetDict()\n",
    "dataset_dict['train'] = datasets.Dataset.from_pandas(train_df)\n",
    "dataset_dict['test'] = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc70253-4773-4571-b5f9-0d63114c2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62a894d-3ac8-4034-9dbf-c3e33d16fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "import numpy as np\n",
    "\n",
    "labels = [ind2label[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [ind2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ind2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7232e9a6-924e-4eea-9ed8-4f09d2daa2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: id, ner_tags, __index_level_0__, tokens. If id, ner_tags, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 38367\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4796' max='4796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4796/4796 1:31:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.111033</td>\n",
       "      <td>0.799878</td>\n",
       "      <td>0.817028</td>\n",
       "      <td>0.808362</td>\n",
       "      <td>0.966429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.814443</td>\n",
       "      <td>0.821719</td>\n",
       "      <td>0.818065</td>\n",
       "      <td>0.967932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: id, ner_tags, __index_level_0__, tokens. If id, ner_tags, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9592\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-2398\n",
      "Configuration saved in ./checkpoint-2398/config.json\n",
      "Model weights saved in ./checkpoint-2398/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2398/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2398/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: id, ner_tags, __index_level_0__, tokens. If id, ner_tags, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9592\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./checkpoint-4796\n",
      "Configuration saved in ./checkpoint-4796/config.json\n",
      "Model weights saved in ./checkpoint-4796/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4796/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4796/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint-4796 (score: 0.10566738247871399).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4796, training_loss=0.12583889814095262, metrics={'train_runtime': 5508.2016, 'train_samples_per_second': 13.931, 'train_steps_per_second': 0.871, 'total_flos': 875733921943158.0, 'train_loss': 0.12583889814095262, 'epoch': 2.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe2904b6-c43c-4287-88eb-8d1831cd6f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-geo',\n",
       "  'score': 0.8744464,\n",
       "  'index': 2,\n",
       "  'word': 'tehran',\n",
       "  'start': 3,\n",
       "  'end': 9},\n",
       " {'entity': 'B-geo',\n",
       "  'score': 0.96626353,\n",
       "  'index': 7,\n",
       "  'word': 'iran',\n",
       "  'start': 25,\n",
       "  'end': 29},\n",
       " {'entity': 'B-org',\n",
       "  'score': 0.64717925,\n",
       "  'index': 10,\n",
       "  'word': 'revolutionary',\n",
       "  'start': 33,\n",
       "  'end': 46},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.8725297,\n",
       "  'index': 11,\n",
       "  'word': 'guards',\n",
       "  'start': 47,\n",
       "  'end': 53},\n",
       " {'entity': 'B-org',\n",
       "  'score': 0.8552725,\n",
       "  'index': 13,\n",
       "  'word': 'general',\n",
       "  'start': 56,\n",
       "  'end': 63},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.8019432,\n",
       "  'index': 14,\n",
       "  'word': 'ya',\n",
       "  'start': 64,\n",
       "  'end': 66},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.6620953,\n",
       "  'index': 15,\n",
       "  'word': '##hya',\n",
       "  'start': 66,\n",
       "  'end': 69},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.61811954,\n",
       "  'index': 16,\n",
       "  'word': 'ra',\n",
       "  'start': 70,\n",
       "  'end': 72},\n",
       " {'entity': 'I-per',\n",
       "  'score': 0.49889368,\n",
       "  'index': 17,\n",
       "  'word': '##him',\n",
       "  'start': 72,\n",
       "  'end': 75},\n",
       " {'entity': 'I-per',\n",
       "  'score': 0.5229419,\n",
       "  'index': 18,\n",
       "  'word': 'sa',\n",
       "  'start': 76,\n",
       "  'end': 78},\n",
       " {'entity': 'I-per',\n",
       "  'score': 0.47986302,\n",
       "  'index': 19,\n",
       "  'word': '##fa',\n",
       "  'start': 78,\n",
       "  'end': 80},\n",
       " {'entity': 'I-per',\n",
       "  'score': 0.5864696,\n",
       "  'index': 20,\n",
       "  'word': '##vi',\n",
       "  'start': 80,\n",
       "  'end': 82},\n",
       " {'entity': 'B-tim',\n",
       "  'score': 0.997255,\n",
       "  'index': 23,\n",
       "  'word': 'saturday',\n",
       "  'start': 90,\n",
       "  'end': 98}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text = ' '.join(tokenized_dataset[\"test\"]['tokens'][0])\n",
    "\n",
    "classifier = pipeline(\"ner\", model=model,tokenizer=tokenizer)\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b8a237-226d-4e7e-a7f6-df58c9558ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight\n",
      "distilbert.embeddings.position_embeddings.weight\n",
      "distilbert.embeddings.LayerNorm.weight\n",
      "distilbert.embeddings.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model2.named_parameters():\n",
    "#     print(name)\n",
    "    if name.startswith(\"distilbert.embeddings\"):\n",
    "        param.requires_grad = False\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39310db8-6d74-4ba4-8c20-b7898c268a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: id, ner_tags, __index_level_0__, tokens. If id, ner_tags, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 38367\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4796' max='4796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4796/4796 1:17:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.112845</td>\n",
       "      <td>0.798892</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.805317</td>\n",
       "      <td>0.965812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.106648</td>\n",
       "      <td>0.813558</td>\n",
       "      <td>0.818101</td>\n",
       "      <td>0.815823</td>\n",
       "      <td>0.967425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: id, ner_tags, __index_level_0__, tokens. If id, ner_tags, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9592\n",
      "  Batch size = 16\n",
      "/Users/tchun/opt/anaconda3/envs/carparts/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./checkpoint-2398\n",
      "Configuration saved in ./checkpoint-2398/config.json\n",
      "Model weights saved in ./checkpoint-2398/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-2398/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2398/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: id, ner_tags, __index_level_0__, tokens. If id, ner_tags, __index_level_0__, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9592\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./checkpoint-4796\n",
      "Configuration saved in ./checkpoint-4796/config.json\n",
      "Model weights saved in ./checkpoint-4796/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-4796/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-4796/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint-4796 (score: 0.10664808750152588).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4796, training_loss=0.12957167883929457, metrics={'train_runtime': 4629.8288, 'train_samples_per_second': 16.574, 'train_steps_per_second': 1.036, 'total_flos': 875733921943158.0, 'train_loss': 0.12957167883929457, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce94de1-431d-45c8-96e3-28e8832d16fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-geo',\n",
       "  'score': 0.92342734,\n",
       "  'index': 2,\n",
       "  'word': 'tehran',\n",
       "  'start': 3,\n",
       "  'end': 9},\n",
       " {'entity': 'B-geo',\n",
       "  'score': 0.9334525,\n",
       "  'index': 7,\n",
       "  'word': 'iran',\n",
       "  'start': 25,\n",
       "  'end': 29},\n",
       " {'entity': 'B-org',\n",
       "  'score': 0.7588199,\n",
       "  'index': 10,\n",
       "  'word': 'revolutionary',\n",
       "  'start': 33,\n",
       "  'end': 46},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.92268586,\n",
       "  'index': 11,\n",
       "  'word': 'guards',\n",
       "  'start': 47,\n",
       "  'end': 53},\n",
       " {'entity': 'B-org',\n",
       "  'score': 0.8368561,\n",
       "  'index': 13,\n",
       "  'word': 'general',\n",
       "  'start': 56,\n",
       "  'end': 63},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.7689902,\n",
       "  'index': 14,\n",
       "  'word': 'ya',\n",
       "  'start': 64,\n",
       "  'end': 66},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.73949224,\n",
       "  'index': 15,\n",
       "  'word': '##hya',\n",
       "  'start': 66,\n",
       "  'end': 69},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.64261,\n",
       "  'index': 16,\n",
       "  'word': 'ra',\n",
       "  'start': 70,\n",
       "  'end': 72},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.62757695,\n",
       "  'index': 17,\n",
       "  'word': '##him',\n",
       "  'start': 72,\n",
       "  'end': 75},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.6266606,\n",
       "  'index': 18,\n",
       "  'word': 'sa',\n",
       "  'start': 76,\n",
       "  'end': 78},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.6982569,\n",
       "  'index': 19,\n",
       "  'word': '##fa',\n",
       "  'start': 78,\n",
       "  'end': 80},\n",
       " {'entity': 'I-org',\n",
       "  'score': 0.51129586,\n",
       "  'index': 20,\n",
       "  'word': '##vi',\n",
       "  'start': 80,\n",
       "  'end': 82},\n",
       " {'entity': 'B-tim',\n",
       "  'score': 0.9971752,\n",
       "  'index': 23,\n",
       "  'word': 'saturday',\n",
       "  'start': 90,\n",
       "  'end': 98}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text = ' '.join(tokenized_dataset[\"test\"]['tokens'][0])\n",
    "\n",
    "classifier = pipeline(\"ner\", model=model2,tokenizer=tokenizer)\n",
    "classifier(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carparts",
   "language": "python",
   "name": "carparts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
